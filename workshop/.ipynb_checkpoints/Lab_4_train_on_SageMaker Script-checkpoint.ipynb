{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Train with custom script on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will fine tune a Huggingface model using Amazon SageMaker. To run training, you can use any of the thousands of models available in Hugging Face and fine-tune them for your specific use case with additional training.\n",
    "\n",
    "Amazon SageMaker enables customers to train, fine-tune, and run inference using Hugging Face models for Natural Language Processing (NLP) on SageMaker. You can use Hugging Face for both training and inference. This functionality is available through the development of Hugging Face AWS Deep Learning Containers. These containers include Hugging Face Transformers, Tokenizers and the Datasets library, which allows you to use these resources for your training and inference jobs. For a list of the available Deep Learning Containers images, see Available Deep Learning Containers Images. These Deep Learning Containers images are maintained and regularly updated with security patches.\n",
    "\n",
    "\n",
    "See this link for more information: https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r lab4path\n",
    "lab4path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create our sagemaker training job we need an HuggingFace Estimator. The Estimator handles all end-to-end Amazon SageMaker training and deployment tasks. In the Estimator we define, which fine-tuning script (entry_point) should be used, which instance_type should be used, which hyperparameters are passed in etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we write a training script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "# Importing stock ml libraries\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "import random\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "    \n",
    "class DistilBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(768, 9)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "# Uncomment to debug GPU stuff\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Parse arguments\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
    "    parser.add_argument(\"--input_file\", type=str, default=\"sagemaker_input_hf.json\")\n",
    "    parser.add_argument(\"--model_id\", type=str)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(args.model_id, truncation=True, do_lower_case=True)\n",
    "    \n",
    "    \n",
    "    print(os.path.join(os.environ[\"SM_CHANNEL_TRAIN\"], args.input_file))\n",
    "    data = pd.read_json(os.path.join(os.environ[\"SM_CHANNEL_TRAIN\"], args.input_file), orient='record', lines=True)\n",
    "    #rename and move\n",
    "    data.columns = ['labels','text']\n",
    "    data = data[['text','labels']]\n",
    "    \n",
    "    \n",
    "    new_df = data.copy()\n",
    "    train_size = 0.8\n",
    "    train_data=new_df.sample(frac=train_size,random_state=200)\n",
    "    test_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    print(\"FULL Dataset: {}\".format(new_df.shape))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "    print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "    training_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
    "    testing_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)\n",
    "    \n",
    "    train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                    'shuffle': True,\n",
    "                    'num_workers': 0\n",
    "                    }\n",
    "\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    testing_loader = DataLoader(testing_set, **test_params)\n",
    "    \n",
    "    \n",
    "\n",
    "    model = DistilBERTClass()\n",
    "    model.to(device)\n",
    "    \n",
    "    def loss_fn(outputs, targets):\n",
    "        return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=float(args.learning_rate))\n",
    "    \n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "\n",
    "        for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            if _%5000==0:\n",
    "                print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    \n",
    "    for epoch in range(args.epochs):\n",
    "        train(epoch)\n",
    "    \n",
    "    # TODO - eval model on test set\n",
    "    \n",
    "    output_model_file = os.path.join(os.environ[\"SM_MODEL_DIR\"],'pytorch_distilbert_news.bin')\n",
    "    output_vocab_file = os.path.join(os.environ[\"SM_MODEL_DIR\"],'vocab_distilbert_news.bin')\n",
    "\n",
    "    torch.save(model, output_model_file)\n",
    "    tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "    print('Model Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Huggingface SageMaker Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Hyperparameters and metric definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={'epochs': 10,                      \n",
    "                 'learning_rate': 3e-5,                \n",
    "                 'model_id':'distilbert-base-uncased'#'allenai/longformer-base-4096'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import sagemaker \n",
    "\n",
    "# This estimator is similar to SageMaker's Tensorflow and Pytorch estimators - compared to Lab 3, you can pass in your training script into\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py', \n",
    "                                   source_dir='src',\n",
    "                                   instance_type='ml.p3.2xlarge',\n",
    "                                   instance_count=1,\n",
    "                                   base_job_name='huggingface',\n",
    "                                   role = sagemaker.get_execution_role(),\n",
    "                                   transformers_version = '4.6',\n",
    "                                   pytorch_version='1.7',\n",
    "                                   py_version='py36',\n",
    "                                   hyperparameters=hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "huggingface_estimator.fit({'train':lab4path}, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Similar to Lab 3, you can deploy the saved model to an Endpoint and make predictions using new text data. Explore the following links for more information about Huggingface on SageMaker:\n",
    "\n",
    "1. https://huggingface.co/transformers/v4.1.1/notebooks.html\n",
    "1. https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html\n",
    "1. https://github.com/huggingface/notebooks/blob/master/sagemaker/10_deploy_model_from_s3/deploy_transformer_model_from_s3.ipynb\n",
    "1. https://github.com/huggingface/notebooks/tree/master/sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
