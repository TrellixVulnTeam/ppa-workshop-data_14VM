{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and process data using pandas\n",
    "\n",
    "The training data we will use today for all labs are inside the folder 'trainingdata' and labels are provided in an Excel file named \"TrainingLabelData.xlsx\". In this lab, we will prep the data for labs 2,3 and 4. \n",
    "\n",
    "Note: If you want to proceed to Labs 2-4 quickly, you can safely click `Run > Run All Cells` from the menu above and continue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels = pd.read_excel('TrainingLabelData.xlsx')\n",
    "len(labels.impactCategoryName.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating labels with a pipe when this is a multi-class dataset (if the dataset is not multi-label, this will still generate a normal multiclass dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_labels = labels.groupby('fileCusip').agg(lambda x : '|'.join(set(x))).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_labels.impactCategoryName = grouped_labels.impactCategoryName.apply(lambda x: '_'.join(x.replace('/','').split(' ')))\n",
    "grouped_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_labels.impactCategoryName.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at some training data .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "training_data_sample = json.load(open('./trainingdata/011415PX8.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Cusip for this record? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_sample['cusip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_sample['file_name'].split('/')[1].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the corresponding label for this record? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_labels[grouped_labels.fileCusip=='13063CEQ9'].impactCategoryName.tolist()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does it look like the proceeds/first page paragraphs has more label-specific information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "' '.join(training_data_sample['first_page_paragraphs'])[:5000].replace(',','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "' '.join(training_data_sample['use_of_proceeds_paragraphs'])[:5000].replace(',','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great, lets prepare a training dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare data for Amazon Comprehend Custom Labels (Lab 2)\n",
    "\n",
    "For training, multi-label mode on Amazon Comprehend supports up to 1 million examples containing up to 100 unique classes.\n",
    "\n",
    "To train a custom classifier, you can provide training data as a two-column CSV file. In it, labels are provided in the first column, and documents are provided in the second.\n",
    "\n",
    "Do not include headers for the individual columns. Including headers in your CSV file may cause runtime errors. Each line of the file contains one or more classes and the text of the training document. More than one class can be indicated by using a delimiter (such as a | ) between each class.\n",
    "\n",
    "```\n",
    "CLASS,Text of document 1\n",
    "CLASS,Text of document 2\n",
    "CLASS|CLASS|CLASS,Text of document 3\n",
    "```\n",
    "\n",
    "For example, the following line belongs to a CSV file that trains a custom classifier to detect genres in movie abstracts:\n",
    "```\n",
    "COMEDY|MYSTERY|SCIENCE_FICTION|TEEN,\"A band of misfit teens become unlikely detectives when they discover troubling clues about their high school English teacher. Could the strange Mrs. Doe be an alien from outer space?\"\n",
    "```\n",
    "\n",
    "\n",
    "The default delimiter between class names is a pipe (|). However, you can use a different character as a delimiter. The delimiter cannot be part of your class name. For example, if your classes are CLASS_1, CLASS_2, and CLASS_3, the underscore (_) is part of the class name. You cannot use then use an underscore as the delimiter for separating class names.\n",
    "\n",
    "\n",
    "Let's create our Comprehend Custom dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "c=0\n",
    "errors=0\n",
    "with open('comprehend_input.csv','w') as f:\n",
    "    for file in glob.glob('./trainingdata/*'):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            c+=1\n",
    "            training_data_sample = json.load(open(file,'r'))\n",
    "            text = ' '.join(training_data_sample['first_page_paragraphs'])[:5000].replace(',','')\n",
    "            proctext = ' '.join(training_data_sample['use_of_proceeds_paragraphs'])[:5000].replace(',','')\n",
    "            filecusip = training_data_sample['file_name'].split('/')[1].split('.')[0]\n",
    "            label = grouped_labels[grouped_labels.fileCusip==filecusip].impactCategoryName.tolist()[0]\n",
    "#             print(label + ',' + text[:10])\n",
    "            # Writing different windows of data as input since Comprehend needs at least 10 examples in each category\n",
    "            # when you have more data, delete the first 4 lines..\n",
    "            f.write(label + ',' + text[:1000] + '\\n')\n",
    "            f.write(label + ',' + text[:2000] + '\\n')\n",
    "            f.write(label + ',' + text[:3000] + '\\n')\n",
    "            f.write(label + ',' + text[:4000] + '\\n')\n",
    "            f.write(label + ',' + text + '\\n')\n",
    "    \n",
    "            f.write('PROCEEDS_PARA,'+ proctext + '\\n')\n",
    "        \n",
    "        except:\n",
    "#             print(file)\n",
    "            errors+=1\n",
    "            print('***')\n",
    "            print(training_data_sample['cusip'])\n",
    "            print('***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we ignore records that don't have a label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat comprehend_input.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab2path = sagemaker.session.Session().upload_data(path='comprehend_input.csv',key_prefix='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ^ Copy this S3 location and move on to Lab 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store lab2path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data for Amazon SageMaker Training (Lab 3)\n",
    "\n",
    "We prepare a dataset that can be ingested using Huggingface on SageMaker for Lab 3. For example, for the Amazon reviews dataset, we could create a dataset that looks like the following:\n",
    "\n",
    "```\n",
    "{\"label\":4,\"review\":\"These are awesome, I like them somjuxh I already ordered another pair\"}\n",
    "{\"label\":4,\"review\":\"This was purchased as a gift for my son who is a GoT lover and when he got it on Christmas day his face lit up with a smile. He wasn't sure anyone would buy it for him or not so this was a double win. Well done Crazy Dog.\"}\n",
    "{\"label\":4,\"review\":\"Nice looking shirt, actually better looking in person than in the picture<br \\/>I got an XLG, (I am not a big guy but don't like clingy, tight clothing). I might have been able to get a LG, but after a washing I think it fits just right and it really didn't shrink much.<br \\/>I ordered it on a Sunday and it was shipped on Monday and delivered on Wednesday So RockWaresUSA did a great job on their end as well.\"}\n",
    "{\"label\":3,\"review\":\"as expected\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, label encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_labels2 = grouped_labels.copy()\n",
    "\n",
    "# converting type of columns to 'category'\n",
    "grouped_labels2['impactCategoryName'] = grouped_labels2['impactCategoryName'].astype('category')\n",
    "# Assigning numerical values and storing in another column\n",
    "\n",
    "grouped_labels2['impactCategoryName'] = grouped_labels2['impactCategoryName'].cat.codes\n",
    "\n",
    "grouped_labels2.impactCategoryName.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a category for the proceeds paragraph\n",
    "proclabel = len(grouped_labels2.impactCategoryName.unique()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Great! Now let's create the dataset for Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldict={}\n",
    "for i, x in enumerate(labels.impactCategoryName.unique().tolist(),1):\n",
    "    labeldict[x]=i\n",
    "\n",
    "labeldict['proclabel']=i+1\n",
    "labeldict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json \n",
    "import re\n",
    "\n",
    "c=0\n",
    "errors=0\n",
    "\n",
    "def write_windows(f,label, text):\n",
    "\n",
    "    words = text.split(' ')\n",
    "    num_words = len(words)\n",
    "    \n",
    "    wpoint = 0\n",
    "\n",
    "    while wpoint*500<len(words):\n",
    "\n",
    "        f.write(json.dumps({\"label\":labeldict[label], \"source\":' '.join(words[wpoint*500:(wpoint+1)*500])}) + '\\n')\n",
    "        wpoint+=1\n",
    "\n",
    "    #f.write(json.dumps({\"label\":label, \"text\":' '.join(words[wpoint*500:])}) + '\\n')\n",
    "    \n",
    "\n",
    "with open('sagemaker_input.json','w') as f:\n",
    "    for file in glob.glob('./trainingdata/*'):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            c+=1\n",
    "            training_data_sample = json.load(open(file,'r'))\n",
    "          \n",
    "            text = re.sub(r'[^A-Za-z0-9 ]+', '', ' '.join(training_data_sample['first_page_paragraphs']))\n",
    "        \n",
    "            proctext = re.sub(r'[^A-Za-z0-9 ]+', '', ' '.join(training_data_sample['use_of_proceeds_paragraphs']))\n",
    "            \n",
    "            filecusip = training_data_sample['file_name'].split('/')[1].split('.')[0]\n",
    "            label = labels[labels.fileCusip==filecusip].impactCategoryName.tolist()[0]\n",
    "            \n",
    "            write_windows(f,label,text)  \n",
    "            \n",
    "            write_windows(f,'proclabel',proctext)   \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            errors+=1\n",
    "            print(training_data_sample['cusip'])\n",
    "            print('***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab3path = sagemaker.session.Session().upload_data(path='sagemaker_input.json',key_prefix='data')\n",
    "%store lab3path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally, prepare data for Lab 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json \n",
    "import re\n",
    "\n",
    "c=0\n",
    "errors=0\n",
    "\n",
    "def write_windows(f,label, text):\n",
    "\n",
    "    words = text.split(' ')\n",
    "    num_words = len(words)\n",
    "    \n",
    "    wpoint = 0\n",
    "    \n",
    "    labellist = [0]*len(labeldict)\n",
    "    \n",
    "    for l in label:\n",
    "        labellist[labeldict[l]]=1\n",
    "\n",
    "    while wpoint*500<len(words):\n",
    "\n",
    "        f.write(json.dumps({\"label\":labellist, \"source\":' '.join(words[wpoint*500:(wpoint+1)*500])}) + '\\n')\n",
    "        wpoint+=1\n",
    "\n",
    "    #f.write(json.dumps({\"label\":label, \"text\":' '.join(words[wpoint*500:])}) + '\\n')\n",
    "    \n",
    "\n",
    "with open('sagemaker_input_hf.json','w') as f:\n",
    "    for file in glob.glob('./trainingdata/*'):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            c+=1\n",
    "            training_data_sample = json.load(open(file,'r'))\n",
    "          \n",
    "            text = re.sub(r'[^A-Za-z0-9 ]+', '', ' '.join(training_data_sample['first_page_paragraphs']))\n",
    "        \n",
    "            proctext = re.sub(r'[^A-Za-z0-9 ]+', '', ' '.join(training_data_sample['use_of_proceeds_paragraphs']))\n",
    "            \n",
    "            filecusip = training_data_sample['file_name'].split('/')[1].split('.')[0]\n",
    "            label = labels[labels.fileCusip==filecusip].impactCategoryName.tolist()\n",
    "\n",
    "            write_windows(f,label,text)           \n",
    "            \n",
    "        except:\n",
    "            errors+=1\n",
    "            print('***')\n",
    "            print(training_data_sample['cusip'])\n",
    "            print('***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab4path = sagemaker.session.Session().upload_data(path='sagemaker_input_hf.json',key_prefix='data')\n",
    "%store lab4path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DONE! Please proceed to Labs 2,3 and 4 in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - If you Ran all cells, make sure there were no errors and the following 3 files are generated successfully:\n",
    "\n",
    "1. comprehend_input.csv\n",
    "2. sagemaker_input.json\n",
    "3. sagemaker_input_hf.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
